{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Basic Modules\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 indicates non-fraudulent claim and 1 indicates fraudulent claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images moved to respective folders.\n"
     ]
    }
   ],
   "source": [
    "#Basic ETL\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"C:/Users/aditya/Desktop/2024/Hackathon_AV/train/train.csv\")\n",
    "\n",
    "# Path to the folder containing the images\n",
    "image_folder = \"C:/Users/aditya/Desktop/2024/Hackathon_AV/train/images\"\n",
    "\n",
    "# Create folders for each label\n",
    "for label in data['label'].unique():\n",
    "    os.makedirs(os.path.join(image_folder, str(label)), exist_ok=True)\n",
    "\n",
    "# Move images to their respective folders based on the label\n",
    "for index, row in data.iterrows():\n",
    "    image_id = row['image_id']\n",
    "    filename = row['filename']\n",
    "    label = row['label']\n",
    "    src = os.path.join(image_folder, filename)\n",
    "    dst = os.path.join(image_folder, str(label), filename)\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "print(\"Images moved to respective folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model -1 Resnet -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "\n",
    "# Define transform to preprocess the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "train_dataset = ImageFolder(root=\"images/\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = torch.tensor([len(train_dataset) / (len(train_dataset) - sum(train_dataset.targets)),\n",
    "                               len(train_dataset) / sum(train_dataset.targets)], dtype=torch.float)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 output classes (0 and 1)\n",
    "\n",
    "# Define loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "  \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'resnet50_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aditya\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "c:\\Users\\aditya\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet50\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn  # Import nn module directly\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(root))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.root, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name  # Return image_name as the image ID\n",
    "\n",
    "# Define transform for test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(root=\"C:/Users/aditya/Desktop/2024/Hackathon_AV/test/images/.\", transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained ResNet50 model\n",
    "model = resnet50(pretrained=False)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Assuming 2 output classes (0 and 1)\n",
    "model.load_state_dict(torch.load('resnet50_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions = {'image_id': [], 'label': []}\n",
    "\n",
    "# Predict labels for test images\n",
    "with torch.no_grad():\n",
    "    for inputs, image_names in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for pred, image_name in zip(predicted, image_names):\n",
    "            predictions['image_id'].append(image_name.split('.')[0])  # Extract image ID from the file name\n",
    "            predictions['label'].append(pred.item())\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "df_predictions.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score 0.6535516551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model -2 VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "\n",
    "# Define transform to preprocess the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "train_dataset = ImageFolder(root=\"images/\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.bincount(train_dataset.targets)\n",
    "class_weights = torch.tensor(class_counts.sum() / (len(class_counts) * class_counts), dtype=torch.float)\n",
    "\n",
    "# Load pre-trained VGG-19 model\n",
    "model = models.vgg19(pretrained=True)\n",
    "# Replace the classifier\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # 2 output classes (0 and 1)\n",
    "\n",
    "# Define loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train only the classifier layers (fine-tuning)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'vgg19_model_with_class_weights_fine_tuned.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import vgg19\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn  # Import nn module directly\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [filename for filename in os.listdir(root) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.root, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name  # Return image_name as the image ID\n",
    "\n",
    "# Define transform for test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(root=\"test/images/\", transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained VGG-19 model\n",
    "model = vgg19(pretrained=False)\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # Assuming 2 output classes (0 and 1)\n",
    "model.load_state_dict(torch.load('vgg19_model_with_class_weights_fine_tuned.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions = {'image_id': [], 'label': []}\n",
    "\n",
    "# Predict labels for test images\n",
    "with torch.no_grad():\n",
    "    for inputs, image_names in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for pred, image_name in zip(predicted, image_names):\n",
    "            predictions['image_id'].append(image_name.split('.')[0])  # Extract image ID from the file name\n",
    "            predictions['label'].append(pred.item())\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "df_predictions.to_csv('submission_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score - 0.6816959152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model -3 VGG19 With augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, ColorJitter\n",
    "\n",
    "# Define transform to preprocess and augment the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(10),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "train_dataset = ImageFolder(root=\"images/\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.bincount(train_dataset.targets)\n",
    "class_weights = torch.tensor(class_counts.sum() / (len(class_counts) * class_counts), dtype=torch.float)\n",
    "\n",
    "# Load pre-trained VGG-19 model\n",
    "model = models.vgg19(pretrained=True)\n",
    "# Replace the classifier\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # 2 output classes (0 and 1)\n",
    "\n",
    "# Define loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train only the classifier layers (fine-tuning)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'vgg19_model_with_augmentation_class_weights_fine_tuned.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score - 0.7148351648 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the files\n",
    "file_paths = [\"submission.csv\", \"submission_1 (1).csv\", \"submission_2.csv\"]\n",
    "data_frames = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# Concatenate the data frames\n",
    "merged_df = pd.concat(data_frames)\n",
    "\n",
    "# Group by image_id and calculate the mode label for each image_id\n",
    "mode_labels = merged_df.groupby('image_id')['label'].agg(lambda x: x.mode()[0])\n",
    "\n",
    "# Create a new DataFrame with image_id and mode label\n",
    "result_df = pd.DataFrame({'image_id': mode_labels.index, 'label': mode_labels.values})\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score - 0.7193250095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model -4 Resnet -50 with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Define transform to preprocess and augment the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "dataset = ImageFolder(root=\"images/\", transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the fully connected layer to match the number of classes in your dataset\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(dataset.classes))  # Use len(dataset.classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Train Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}, Val Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'resnet50_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet50\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn  # Import nn module directly\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [filename for filename in os.listdir(root) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.root, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name  # Return image_name as the image ID\n",
    "\n",
    "# Define transform for test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(root=\"test/images/\", transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained ResNet50 model\n",
    "model = resnet50(pretrained=False)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Assuming 2 output classes (0 and 1)\n",
    "model.load_state_dict(torch.load('resnet50_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions = {'image_id': [], 'label': []}\n",
    "\n",
    "# Predict labels for test images\n",
    "with torch.no_grad():\n",
    "    for inputs, image_names in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for pred, image_name in zip(predicted, image_names):\n",
    "            predictions['image_id'].append(image_name.split('.')[0])  # Extract image ID from the file name\n",
    "            predictions['label'].append(pred.item())\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "df_predictions.to_csv('submission_resnet50.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score -0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model  5 Inception V3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "\n",
    "# Define transform to preprocess and augment the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "dataset = ImageFolder(root=\"images/\", transform=transform)\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = np.bincount(dataset.targets)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Determine the minority class\n",
    "minority_class = np.argmin(class_counts)\n",
    "\n",
    "# Oversample the minority class to balance the dataset\n",
    "oversampled_indices = []\n",
    "for idx, (data, target) in enumerate(dataset):\n",
    "    if target == minority_class:\n",
    "        oversampled_indices.append(idx)\n",
    "\n",
    "oversampled_dataset = torch.utils.data.Subset(dataset, oversampled_indices)\n",
    "balanced_dataset = torch.utils.data.ConcatDataset([dataset, oversampled_dataset])\n",
    "\n",
    "# Define DataLoader for the balanced dataset\n",
    "train_loader = DataLoader(balanced_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load pre-trained Inception v3 model\n",
    "model = inception_v3(pretrained=True)\n",
    "\n",
    "# Freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the fully connected layer to match the number of classes in your dataset\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs)  # Extract primary logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Train Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    # Validate the model\n",
    "    # (Note: Without a separate validation dataset, you can skip this part or use cross-validation)\n",
    "    \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'inception_v3_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import inception_v3\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn  # Import nn module directly\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [filename for filename in os.listdir(root) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.root, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name  # Return image_name as the image ID\n",
    "\n",
    "# Define transform for test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # InceptionV3 expects 299x299 images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(root=\"test/images/\", transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained InceptionV3 model\n",
    "model = inception_v3(pretrained=False)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Assuming 2 output classes (0 and 1)\n",
    "model.load_state_dict(torch.load('inception_v3_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions = {'image_id': [], 'label': []}\n",
    "\n",
    "# Predict labels for test images\n",
    "with torch.no_grad():\n",
    "    for inputs, image_names in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for pred, image_name in zip(predicted, image_names):\n",
    "            predictions['image_id'].append(image_name.split('.')[0])  # Extract image ID from the file name\n",
    "            predictions['label'].append(pred.item())\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "df_predictions.to_csv('submission_inception_v3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your score for this submission is : 0.5826018090692893."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode Based Approach\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the files\n",
    "file_paths = [\"submission.csv\", \"submission_1 (1).csv\", \"submission_2.csv\",\n",
    "\"submission_inception_v3.csv\",\"submission_resnet50.csv\"]\n",
    "data_frames = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# Concatenate the data frames\n",
    "merged_df = pd.concat(data_frames)\n",
    "\n",
    "# Group by image_id and calculate the mode label for each image_id\n",
    "mode_labels = merged_df.groupby('image_id')['label'].agg(lambda x: x.mode()[0])\n",
    "\n",
    "# Create a new DataFrame with image_id and mode label\n",
    "result_df = pd.DataFrame({'image_id': mode_labels.index, 'label': mode_labels.values})\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('result_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score -0.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model -6 Dense Net 169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import densenet169\n",
    "import numpy as np\n",
    "\n",
    "# Define transform to preprocess and augment the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "dataset = ImageFolder(root=\"images/\", transform=transform)\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = np.bincount(dataset.targets)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Determine the minority class\n",
    "minority_class = np.argmin(class_counts)\n",
    "\n",
    "# Oversample the minority class to balance the dataset\n",
    "oversampled_indices = []\n",
    "for idx, (data, target) in enumerate(dataset):\n",
    "    if target == minority_class:\n",
    "        oversampled_indices.append(idx)\n",
    "\n",
    "oversampled_dataset = torch.utils.data.Subset(dataset, oversampled_indices)\n",
    "balanced_dataset = torch.utils.data.ConcatDataset([dataset, oversampled_dataset])\n",
    "\n",
    "# Define DataLoader for the balanced dataset\n",
    "train_loader = DataLoader(balanced_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load pre-trained DenseNet169 model\n",
    "model = densenet169(pretrained=True)\n",
    "\n",
    "# Freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the fully connected layer to match the number of classes in your dataset\n",
    "num_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 12\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Train Acc: {100.*correct/total:.2f}%\")\n",
    "\n",
    "    # Validate the model\n",
    "    # (Note: Without a separate validation dataset, you can skip this part or use cross-validation)\n",
    "    \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'densenet169_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import densenet169\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn  # Import nn module directly\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [filename for filename in os.listdir(root) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.root, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name  # Return image_name as the image ID\n",
    "\n",
    "# Define transform for test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match DenseNet169 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(root=\"test/images/\", transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained DenseNet169 model\n",
    "model = densenet169(pretrained=False)\n",
    "num_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_features, 2)  # Assuming 2 output classes (0 and 1)\n",
    "model.load_state_dict(torch.load('densenet169_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions = {'image_id': [], 'label': []}\n",
    "\n",
    "# Predict labels for test images\n",
    "with torch.no_grad():\n",
    "    for inputs, image_names in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for pred, image_name in zip(predicted, image_names):\n",
    "            predictions['image_id'].append(image_name.split('.')[0])  # Extract image ID from the file name\n",
    "            predictions['label'].append(pred.item())\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "df_predictions.to_csv('submission_densenet169.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mode Based Approach\n",
    "# Mode Based Approach\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the files\n",
    "file_paths = [\"submission_2.csv\",\"submission_resnet50.csv\",\"submission_densenet169.csv\",\"submission_vgg19_hope.csv\"]\n",
    "data_frames = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# Concatenate the data frames\n",
    "merged_df = pd.concat(data_frames)\n",
    "\n",
    "# Group by image_id and calculate the mode label for each image_id\n",
    "mode_labels = merged_df.groupby('image_id')['label'].agg(lambda x: x.mode()[0])\n",
    "\n",
    "# Create a new DataFrame with image_id and mode label\n",
    "result_df = pd.DataFrame({'image_id': mode_labels.index, 'label': mode_labels.values})\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('result_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7 VGG-19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define transform to preprocess and augment the images\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "dataset = ImageFolder(root=\"images/\", transform=transform_train)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Load pre-trained VGG-19 model\n",
    "model = models.vgg19(pretrained=True)\n",
    "\n",
    "# Freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # 2 output classes (0 and 1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)  # Only optimize classifier parameters\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "best_f1 = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    val_f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "    # Save the model with the best F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'vgg19_model_fine_tuned_with_augmentation.pth')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation F1 Score: {val_f1:.4f}')\n",
    "\n",
    "print('Training finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import vgg19\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn  # Import nn module directly\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [filename for filename in os.listdir(root) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.root, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name  # Return image_name as the image ID\n",
    "\n",
    "# Define transform for test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match VGG19 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(root=\"test/images/\", transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained VGG19 model\n",
    "model = vgg19(pretrained=False)\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # Assuming 2 output classes (0 and 1)\n",
    "model.load_state_dict(torch.load('vgg19_model_fine_tuned_with_augmentation.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions = {'image_id': [], 'label': []}\n",
    "\n",
    "# Predict labels for test images\n",
    "with torch.no_grad():\n",
    "    for inputs, image_names in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for pred, image_name in zip(predicted, image_names):\n",
    "            predictions['image_id'].append(image_name.split('.')[0])  # Extract image ID from the file name\n",
    "            predictions['label'].append(pred.item())\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "df_predictions.to_csv('submission_vgg19_hope.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model VGG With better augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define transform to preprocess and augment the images\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset\n",
    "dataset = ImageFolder(root=\"images/\", transform=transform_train)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.bincount([dataset.targets[i] for i in train_indices])\n",
    "total_samples = len(train_dataset)\n",
    "class_weights = torch.tensor([total_samples / (len(class_counts) * count) for count in class_counts], dtype=torch.float)\n",
    "\n",
    "# Load pre-trained VGG-19 model\n",
    "model = models.vgg19(pretrained=True)\n",
    "\n",
    "# Freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # 2 output classes (0 and 1)\n",
    "\n",
    "# Define loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.0001)  # Only optimize classifier parameters\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "best_f1 = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    val_f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "    # Save the model with the best F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'vgg19_model_fine_tuned_with_augmentation.pth')\n",
    "\n",
    "    print(f'Validation F1 Score: {val_f1:.4f}')\n",
    "\n",
    "print('Training finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import vgg19\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn  # Import nn module directly\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [filename for filename in os.listdir(root) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.root, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name  # Return image_name as the image ID\n",
    "\n",
    "# Define transform for test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match VGG19 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = CustomTestDataset(root=\"test/images/\", transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the trained VGG19 model\n",
    "model = vgg19(pretrained=False)\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 2)  # Assuming 2 output classes (0 and 1)\n",
    "model.load_state_dict(torch.load('vgg19_model_fine_tuned_with_augmentation.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions = {'image_id': [], 'label': []}\n",
    "\n",
    "# Predict labels for test images\n",
    "with torch.no_grad():\n",
    "    for inputs, image_names in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for pred, image_name in zip(predicted, image_names):\n",
    "            predictions['image_id'].append(image_name.split('.')[0])  # Extract image ID from the file name\n",
    "            predictions['label'].append(pred.item())\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "df_predictions.to_csv('submission_vgg19_hope_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Mode Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the files\n",
    "file_paths = [\"submission_2.csv\",\n",
    "              \"submission_vgg19_hope_1.csv\",\"submission_vgg19_hope.csv\",\"submission.csv\"]\n",
    "data_frames = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# Concatenate the data frames\n",
    "merged_df = pd.concat(data_frames)\n",
    "\n",
    "# Group by image_id and take the minimum label for each image_id\n",
    "min_labels = merged_df.groupby('image_id')['label'].max()\n",
    "\n",
    "# Create a new DataFrame with image_id and minimum label\n",
    "result_df = pd.DataFrame({'image_id': min_labels.index, 'label': min_labels.values})\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('result_minimum.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
